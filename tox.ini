[tox]
skipsdist = True
envlist = py36,py37,py38,py39,flake8

[testenv:flake8]
description = flake8 code checks
basepython = python3.8
skip_install = true
commands = flake8 --select=E,W,F --ignore=W504,E741 --max-line-length 99 \
  dbt
deps =
  -rdev_requirements.txt

[testenv:{unit,py36,py37,py38,py39,py}]
description = unit testing
skip_install = true
passenv = DBT_* PYTEST_ADDOPTS
commands = {envpython} -m pytest {posargs} tests/unit
deps =
  -rdev_requirements.txt
  -e.[all]

[testenv:{integration,py36,py37,py38,py39,py}-{apache_spark,databricks_http,databricks_cluster,databricks_endpoint}]
description = adapter plugin integration testing
skip_install = true
passenv = DBT_DATABRICKS_HOST_NAME DBT_DATABRICKS_CLUSTER_NAME DBT_DATABRICKS_TOKEN DBT_INVOCATION_ENV ODBC_DRIVER
commands =
  apache_spark: {envpython} -m pytest -v tests/specs/spark-thrift.dbtspec
  apache_spark: {envpython} -m pytest {posargs} -m profile_apache_spark tests/integration
  databricks_http: {envpython} -m pytest -v tests/specs/spark-databricks-http.dbtspec
  databricks_cluster: {envpython} -m pytest -v tests/specs/spark-databricks-odbc-cluster.dbtspec
  databricks_cluster: {envpython} -m pytest {posargs} -m profile_databricks_cluster tests/integration
  databricks_endpoint: {envpython} -m pytest -v tests/specs/spark-databricks-odbc-sql-endpoint.dbtspec
  databricks_endpoint: {envpython} -m pytest {posargs} -m profile_databricks_sql_endpoint tests/integration
deps =
  -rdev_requirements.txt
  -e.[all]
  
[pytest]
env_files =
    test.env
testpaths =
    tests/unit
    tests/integration
